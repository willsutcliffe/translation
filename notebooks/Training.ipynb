{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dddc3574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df1 = pd.read_csv(\"en_ru_1m_yandex.csv\")\n",
    "df3 = pd.read_csv(\"en_ru.csv\")\n",
    "df4 = pd.read_csv(\"en_ru_2.csv\")\n",
    "df5 = pd.read_csv(\"news.csv\")\n",
    "\n",
    "df = pd.concat([df1,df3,df4,df5])\n",
    "df = df[['ru', 'en']]\n",
    "df = df.reset_index()\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['lenru'] = df.ru.apply(lambda x:len(x.split()))\n",
    "df['lenen'] = df.en.apply(lambda x:len(x.split()))\n",
    "df = df.loc[ (df.lenen < 60) & (df.lenru < 60)]\n",
    "df.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6586a0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ru</th>\n",
       "      <th>en</th>\n",
       "      <th>lenru</th>\n",
       "      <th>lenen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>такое развитие характера гарри может разочаров...</td>\n",
       "      <td>this new development in harry's character may ...</td>\n",
       "      <td>28</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>решение суда (группа вернулась под крыло к ele...</td>\n",
       "      <td>a nondisclosure clause in the final settlement...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>когда тебе 18 или 19 лет, легко перенимать бан...</td>\n",
       "      <td>when you're 18 or 19 years old, you have that ...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>а сейчас куча триьютов тем же самым black sabb...</td>\n",
       "      <td>now you have black sabbath and kiss tribute al...</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>я был единственным, кто занялся копированием д...</td>\n",
       "      <td>i was the one who sat down and copied them.</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511005</th>\n",
       "      <td>193207</td>\n",
       "      <td>их достижения остаются одними из самых значите...</td>\n",
       "      <td>the ability to begin to see more clearly than ...</td>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511006</th>\n",
       "      <td>193208</td>\n",
       "      <td>в тоже время революционное поколение зумы все ...</td>\n",
       "      <td>forced to reckon with your own vulnerability, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511007</th>\n",
       "      <td>193209</td>\n",
       "      <td>в регионе, который преклоняется перед пожилыми...</td>\n",
       "      <td>mandela epitomized this rare gift.</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511008</th>\n",
       "      <td>193210</td>\n",
       "      <td>трое из десяти южноафриканцев младше 15 лет, ч...</td>\n",
       "      <td>how else could he have personally invited one ...</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511009</th>\n",
       "      <td>193211</td>\n",
       "      <td>каким-то образом зума должен найти способ уваж...</td>\n",
       "      <td>of course, behind mandela’s generous spirit wa...</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2480312 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          index                                                 ru  \\\n",
       "0             0  такое развитие характера гарри может разочаров...   \n",
       "1             1  решение суда (группа вернулась под крыло к ele...   \n",
       "2             2  когда тебе 18 или 19 лет, легко перенимать бан...   \n",
       "3             3  а сейчас куча триьютов тем же самым black sabb...   \n",
       "4             4  я был единственным, кто занялся копированием д...   \n",
       "...         ...                                                ...   \n",
       "2511005  193207  их достижения остаются одними из самых значите...   \n",
       "2511006  193208  в тоже время революционное поколение зумы все ...   \n",
       "2511007  193209  в регионе, который преклоняется перед пожилыми...   \n",
       "2511008  193210  трое из десяти южноафриканцев младше 15 лет, ч...   \n",
       "2511009  193211  каким-то образом зума должен найти способ уваж...   \n",
       "\n",
       "                                                        en  lenru  lenen  \n",
       "0        this new development in harry's character may ...     28     41  \n",
       "1        a nondisclosure clause in the final settlement...     24     24  \n",
       "2        when you're 18 or 19 years old, you have that ...     15     15  \n",
       "3        now you have black sabbath and kiss tribute al...     11      9  \n",
       "4              i was the one who sat down and copied them.      9     10  \n",
       "...                                                    ...    ...    ...  \n",
       "2511005  the ability to begin to see more clearly than ...     10     27  \n",
       "2511006  forced to reckon with your own vulnerability, ...     22     31  \n",
       "2511007                 mandela epitomized this rare gift.     21      5  \n",
       "2511008  how else could he have personally invited one ...     18     24  \n",
       "2511009  of course, behind mandela’s generous spirit wa...     34     11  \n",
       "\n",
       "[2480312 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c330c467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a161600",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b67ac7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4287ce2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ffbd61ec79cfe0b0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-ffbd61ec79cfe0b0/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b59ee5b81e4456bbacdf35397c726f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05a8e0afcc04e82b63ff889e857adc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/datasets/download/streaming_download_manager.py:695: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-ffbd61ec79cfe0b0/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2a99e49a4e4dd18680dd08a2ca0d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'index', 'ru', 'en', 'lenru', 'lenen'],\n",
      "        num_rows: 2480312\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc7d0c7ff9f434fbb087ba7ff48b7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8819bcc1d65d4de8ba740bc6b4f010bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4920a15f2bff442dbc178c50469b493c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading source.spm:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b0dbcd65864985866ecba54471769b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading target.spm:   0%|          | 0.00/784k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87333bf1459946a0be4be260827e05d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/models/marian/tokenization_marian.py:198: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b466e81bdd407fb51b5dea2932c635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2456 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc5b6ea83da46c294f5a571107fc555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from translation.models.encoder import Encoder\n",
    "from translation.models.decoder import Decoder\n",
    "from translation.models.transformers import Transformer\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "from translation.models.transformers import Transformer\n",
    "from translation.models.encoder import Encoder\n",
    "from translation.models.decoder import Decoder\n",
    "\n",
    "\n",
    "def preprocess_function(batch,lang='ru-en'):\n",
    "    l1 = lang.split('-')[0]\n",
    "    l2 = lang.split('-')[1]\n",
    "    model_inputs = tokenizer(\n",
    "        batch[l1], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[l2], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "raw_dataset = load_dataset('csv',\n",
    "        data_files='test.csv')\n",
    "print(raw_dataset)\n",
    "lang_pair = \"ru-en\"\n",
    "l1 = lang_pair.split('-')[0]\n",
    "l2 = lang_pair.split('-')[1]\n",
    "lang_pair = \"ru-en\"\n",
    "model_checkpoint = f\"Helsinki-NLP/opus-mt-{lang_pair}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "split = raw_dataset['train'].train_test_split(test_size=0.01, seed=42)\n",
    "max_input_length = 60\n",
    "max_target_length = 60\n",
    "\n",
    "tokenized_datasets = split.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(0, 5)])\n",
    "batch.keys()\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=256,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=6, pin_memory=True,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    batch_size=256,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=6, pin_memory=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4385dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2455508\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 24804\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b94834d",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b819b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using encoder without a target.\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(62519, 512)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): DecoderBlock(\n",
       "      (linear1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mhatt1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mhatt2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (nn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate=none)\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (linear1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mhatt1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mhatt2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (nn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate=none)\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): DecoderBlock(\n",
       "      (linear1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mhatt1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mhatt2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (nn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate=none)\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): DecoderBlock(\n",
       "      (linear1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mhatt1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mhatt2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (nn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate=none)\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (linear): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=512, out_features=62519, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from translation.models.transformers import Transformer\n",
    "from translation.models.encoder import Encoder\n",
    "from translation.models.decoder import Decoder\n",
    "\n",
    "dkey = 64\n",
    "dmodel = 512\n",
    "nheads = 8\n",
    "nlayers = 4\n",
    "dropout = 0.1\n",
    "\n",
    "encoder = Encoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                  max_len=60,\n",
    "                  d_key=64,\n",
    "                  d_model=512,\n",
    "                  n_heads=8,\n",
    "                  n_layers=4,\n",
    "                  dropout_prob=0.1)\n",
    "decoder = Decoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                  max_len=60,\n",
    "                  d_key=64,\n",
    "                  d_model=512,\n",
    "                  n_heads=8,\n",
    "                  n_layers=4,\n",
    "                  dropout_prob=0.1)\n",
    "transformer = Transformer(encoder, decoder)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9688071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5911c917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "Transformer                                   --\n",
       "├─Encoder: 1-1                                --\n",
       "│    └─Embedding: 2-1                         32,009,728\n",
       "│    └─PositionalEncoding: 2-2                --\n",
       "│    │    └─Dropout: 3-1                      --\n",
       "│    └─Sequential: 2-3                        --\n",
       "│    │    └─EncoderBlock: 3-2                 3,152,384\n",
       "│    │    └─EncoderBlock: 3-3                 3,152,384\n",
       "│    │    └─EncoderBlock: 3-4                 3,152,384\n",
       "│    │    └─EncoderBlock: 3-5                 3,152,384\n",
       "│    └─LayerNorm: 2-4                         1,024\n",
       "├─Decoder: 1-2                                --\n",
       "│    └─Embedding: 2-5                         32,009,728\n",
       "│    └─PositionalEncoding: 2-6                --\n",
       "│    │    └─Dropout: 3-6                      --\n",
       "│    └─Sequential: 2-7                        --\n",
       "│    │    └─DecoderBlock: 3-7                 4,204,032\n",
       "│    │    └─DecoderBlock: 3-8                 4,204,032\n",
       "│    │    └─DecoderBlock: 3-9                 4,204,032\n",
       "│    │    └─DecoderBlock: 3-10                4,204,032\n",
       "│    └─LayerNorm: 2-8                         1,024\n",
       "│    └─Linear: 2-9                            32,072,247\n",
       "======================================================================\n",
       "Total params: 125,519,415\n",
       "Trainable params: 125,519,415\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1007ad39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate(input_sentence):\n",
    "\n",
    "    encoder_input = tokenizer(input_sentence, return_tensors='pt',max_length=max_input_length, truncation=True).to(device)\n",
    "\n",
    "    encoder_output = encoder(encoder_input['input_ids'], encoder_input['attention_mask'])\n",
    "\n",
    "    decoder_input_ids = torch.tensor([[ int(tokenizer.vocab_size)]], device=device)\n",
    "    decoder_attn_mask = torch.ones_like(decoder_input_ids, device=device)\n",
    "\n",
    "    for _ in range(60):\n",
    "        decoder_output = decoder(\n",
    "            encoder_output,\n",
    "            decoder_input_ids,\n",
    "            encoder_input['attention_mask'],\n",
    "            decoder_attn_mask,\n",
    "        )\n",
    "\n",
    "\n",
    "        prediction_id = torch.argmax(decoder_output[:, -1, :], axis=-1)\n",
    "        decoder_input_ids = torch.hstack((decoder_input_ids, prediction_id.view(1, 1)))\n",
    "        decoder_attn_mask = torch.ones_like(decoder_input_ids)\n",
    "\n",
    "        if prediction_id == 0:\n",
    "            break\n",
    "  \n",
    "    translation = tokenizer.decode(decoder_input_ids[0, 1:])\n",
    "\n",
    "    return(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ba159fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(),lr=1e-4)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold=0.04, verbose=True, patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a65b52",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b48ce324",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cf03784e7e4235a7504e0a3051bfbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2435acdd370e4d4cae29dcee34e6d7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de093c6952d4defa50bf2eee2d0e48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ba115051fa452d94774e38ef9d9e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import evaluate \n",
    "bleu_metric = evaluate.load('bleu')\n",
    "#bleu_metric = load_metric(\"sacrebleu\")\n",
    "bert_metric = load_metric(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfaf7b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2480312"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5da2231",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa93ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, valid_loader, epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold=0.05, threshold_mode='rel')\n",
    "    for it in range(epochs):\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        batch_count = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # initalize gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get inputs\n",
    "            enc_input = batch['input_ids']\n",
    "            enc_mask = batch['attention_mask']\n",
    "            targets = batch['labels']\n",
    "\n",
    "            # shift targets forwards to get decoder_input\n",
    "            dec_input = targets.clone().detach()\n",
    "            dec_input = torch.roll(dec_input, shifts=1, dims=1)\n",
    "            dec_input[:, 0] = int(tokenizer.vocab_size)\n",
    "\n",
    "            # convert all -100 to pad token id\n",
    "            dec_input = dec_input.masked_fill(\n",
    "              dec_input == -100, tokenizer.pad_token_id)\n",
    "\n",
    "            # make decoder input mask\n",
    "            dec_mask = torch.ones_like(dec_input)\n",
    "            dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "\n",
    "            # forward pass note using f16 with torch.amp\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "                loss = criterion(outputs.transpose(2, 1), targets)\n",
    "\n",
    "            # Backward prop and grad descent step\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            \n",
    "            scaler.update()\n",
    "            # if using \n",
    "            #scheduler.step(optimizer)\n",
    "            train_loss.append(loss.item())\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_count%200 == 0:\n",
    "                print( \"Progress: \", 100*batch_count/ (len(df)*0.99/256.), \" %\")\n",
    "\n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        \n",
    "        # Evaluate the perf on validation\n",
    "        for batch in valid_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            enc_input = batch['input_ids']\n",
    "            enc_mask = batch['attention_mask']\n",
    "            targets = batch['labels']\n",
    "\n",
    "            dec_input = targets.clone().detach()\n",
    "            dec_input = torch.roll(dec_input, shifts=1, dims=1)\n",
    "            dec_input[:, 0] = int(tokenizer.vocab_size)\n",
    "\n",
    "            dec_input = dec_input.masked_fill(\n",
    "              dec_input == -100, tokenizer.pad_token_id)\n",
    "\n",
    "            dec_mask = torch.ones_like(dec_input)\n",
    "            dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "\n",
    "            outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "            loss = criterion(outputs.transpose(2, 1), targets)\n",
    "            test_loss.append(loss.item())\n",
    "        test_loss = np.mean(test_loss)\n",
    "\n",
    "        # Save losses\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        translations = []\n",
    "        targets = []\n",
    "        bertscores = []\n",
    "        bleuscores = []\n",
    "        torch.save(transformer.state_dict(), f\"model_{it}.pt\")\n",
    "        \n",
    "        # compute BERT and BLEU scores on 200 examples \n",
    "        for  i in range(200):\n",
    "            translations.append(translate(split['test'][i][l1])[:-4])\n",
    "            targets.append(split['test'][i][l2])\n",
    "            score =bert_metric.compute(predictions=[translations[i]], references=[targets[i]], lang=\"ru\")['f1']\n",
    "            bertscores.append(score)\n",
    "            try:\n",
    "                score =bleu_metric.compute(predictions=[translations[i]], references=[[targets[i]]])['bleu']\n",
    "                bleuscores.append(score)\n",
    "            except:\n",
    "                bleuscores.append(0)\n",
    "\n",
    "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "          Test Loss: {test_loss:.4f}, Duration: {dt}')\n",
    "\n",
    "        print(f\"Mean BERT {np.mean(bertscores)}, Mean Bleu: {np.mean(bleuscores)}\" )\n",
    "\n",
    "        training_data.append({\n",
    "            'Epoch' : it+1,\n",
    "            'Train_loss' : train_loss,\n",
    "            'Val_loss' : test_loss,\n",
    "            'Val_BERT_score' : np.mean(bertscores),\n",
    "            'Val_BLEU_score': np.mean(bleuscores),\n",
    "            'time per epoch' : dt,\n",
    "            'd_keys' : dkey,\n",
    "            'd_model' : dmodel,\n",
    "            'n_heads' : nheads,\n",
    "             'n_layers' :nlayers,\n",
    "            'dropout_prob' :dropout\n",
    "            \n",
    "        })\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4df646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses, test_losses = train(\n",
    "    transformer, criterion, optimizer, train_loader, valid_loader, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(training_data)\n",
    "df.to_csv(\"Training_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
