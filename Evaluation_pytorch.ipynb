{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dddc3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file = \"en_fr\"\n",
    "df1 = pd.read_csv(\"/home/sutclw/Work/Preparation/Projects/translation/Notebooks/en_ru.csv\")\n",
    "df2 = pd.read_csv(\"/home/sutclw/Work/Preparation/Projects/translation/Notebooks/en_ru_2.csv\")\n",
    "\n",
    "#df = df1.append(df2)\n",
    "df1.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6586a0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ru</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Давайте что-нибудь попробуем!</td>\n",
       "      <td>Let's try something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Мне пора идти спать.</td>\n",
       "      <td>I have to go to sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Что ты делаешь?</td>\n",
       "      <td>What are you doing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Что ты делаешь?</td>\n",
       "      <td>What do you make?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Что ты делаешь?</td>\n",
       "      <td>What're you doing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656928</th>\n",
       "      <td>656929</td>\n",
       "      <td>Лифт всё ещё не работает?</td>\n",
       "      <td>Is the elevator still out of order?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656929</th>\n",
       "      <td>656930</td>\n",
       "      <td>Я сказал тебе, чтобы ты оставался здесь.</td>\n",
       "      <td>I told you to stay here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656930</th>\n",
       "      <td>656931</td>\n",
       "      <td>Я сказал тебе, чтобы ты оставалась здесь.</td>\n",
       "      <td>I told you to stay here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656931</th>\n",
       "      <td>656932</td>\n",
       "      <td>Я сказал вам, чтобы вы оставались здесь.</td>\n",
       "      <td>I told you to stay here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656932</th>\n",
       "      <td>656933</td>\n",
       "      <td>Джинсы ко всему подходят.</td>\n",
       "      <td>Jeans go with everything.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>656933 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                         ru  \\\n",
       "0                1              Давайте что-нибудь попробуем!   \n",
       "1                2                       Мне пора идти спать.   \n",
       "2                3                            Что ты делаешь?   \n",
       "3                4                            Что ты делаешь?   \n",
       "4                5                            Что ты делаешь?   \n",
       "...            ...                                        ...   \n",
       "656928      656929                  Лифт всё ещё не работает?   \n",
       "656929      656930   Я сказал тебе, чтобы ты оставался здесь.   \n",
       "656930      656931  Я сказал тебе, чтобы ты оставалась здесь.   \n",
       "656931      656932   Я сказал вам, чтобы вы оставались здесь.   \n",
       "656932      656933                  Джинсы ко всему подходят.   \n",
       "\n",
       "                                         en  \n",
       "0                      Let's try something.  \n",
       "1                    I have to go to sleep.  \n",
       "2                       What are you doing?  \n",
       "3                         What do you make?  \n",
       "4                        What're you doing?  \n",
       "...                                     ...  \n",
       "656928  Is the elevator still out of order?  \n",
       "656929             I told you to stay here.  \n",
       "656930             I told you to stay here.  \n",
       "656931             I told you to stay here.  \n",
       "656932            Jeans go with everything.  \n",
       "\n",
       "[656933 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330c467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a161600",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67ac7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\")\n",
    "\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c48941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561f424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4287ce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-84210adc254c38ba\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/sutclw/.cache/huggingface/datasets/csv/default-84210adc254c38ba/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sutclw/.local/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/sutclw/.cache/huggingface/datasets/csv/default-84210adc254c38ba/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0.1', 'Unnamed: 0', 'ru', 'en'],\n",
      "        num_rows: 656933\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sutclw/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from translation.models.encoder import Encoder\n",
    "from translation.models.decoder import Decoder\n",
    "from translation.models.transformers import Transformer\n",
    "from translation.training.littrainer import  LitTrainer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import pytorch_lightning as pl\n",
    "from translation.models.transformers import Transformer\n",
    "from translation.models.encoder import Encoder\n",
    "from translation.models.decoder import Decoder\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "def preprocess_function(batch,lang='en-ru'):\n",
    "    l1 = lang.split('-')[0]\n",
    "    l2 = lang.split('-')[1]\n",
    "    model_inputs = tokenizer(\n",
    "        batch[l1], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[l2], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Press the green button in the gutter to run the script.\n",
    "\n",
    "    print('PyCharm')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "raw_dataset = load_dataset('csv',\n",
    "        data_files='test.csv')\n",
    "print(raw_dataset)\n",
    "lang_pair = \"en-ru\"\n",
    "l1 = lang_pair.split('-')[0]\n",
    "l2 = lang_pair.split('-')[1]\n",
    "lang_pair = \"en-ru\"\n",
    "model_checkpoint = f\"Helsinki-NLP/opus-mt-{lang_pair}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "split = raw_dataset['train'].train_test_split(test_size=0.05, seed=42)\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "tokenized_datasets = split.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(0, 5)])\n",
    "batch.keys()\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=4, pin_memory=True,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    batch_size=64,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4385dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 624086\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 32847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b94834d",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b819b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using encoder without a target.\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(62519, 128)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): DecoderBlock(\n",
       "      (linear1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mhatt1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mhatt2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (nn): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (linear1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mhatt1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mhatt2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (nn): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): DecoderBlock(\n",
       "      (linear1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mhatt1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mhatt2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (nn): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): DecoderBlock(\n",
       "      (linear1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mhatt1): MultiHeadAttention(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mhatt2): MultiHeadAttention(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (nn): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (linear): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=128, out_features=62519, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                  max_len=128,\n",
    "                  d_key=16,\n",
    "                  d_model=128,\n",
    "                  n_heads=8,\n",
    "                  n_layers=4,\n",
    "                  dropout_prob=0.1)\n",
    "decoder = Decoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                  max_len=128,\n",
    "                  d_key=16,\n",
    "                  d_model=128,\n",
    "                  n_heads=8,\n",
    "                  n_layers=4,\n",
    "                  dropout_prob=0.1)\n",
    "transformer = Transformer(encoder, decoder)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5911c917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "Transformer                                   --\n",
       "├─Encoder: 1-1                                --\n",
       "│    └─Embedding: 2-1                         8,002,432\n",
       "│    └─PositionalEncoding: 2-2                --\n",
       "│    │    └─Dropout: 3-1                      --\n",
       "│    └─Sequential: 2-3                        --\n",
       "│    │    └─EncoderBlock: 3-2                 198,272\n",
       "│    │    └─EncoderBlock: 3-3                 198,272\n",
       "│    │    └─EncoderBlock: 3-4                 198,272\n",
       "│    │    └─EncoderBlock: 3-5                 198,272\n",
       "│    └─LayerNorm: 2-4                         256\n",
       "├─Decoder: 1-2                                --\n",
       "│    └─Embedding: 2-5                         8,002,432\n",
       "│    └─PositionalEncoding: 2-6                --\n",
       "│    │    └─Dropout: 3-6                      --\n",
       "│    └─Sequential: 2-7                        --\n",
       "│    │    └─DecoderBlock: 3-7                 264,576\n",
       "│    │    └─DecoderBlock: 3-8                 264,576\n",
       "│    │    └─DecoderBlock: 3-9                 264,576\n",
       "│    │    └─DecoderBlock: 3-10                264,576\n",
       "│    └─LayerNorm: 2-8                         256\n",
       "│    └─Linear: 2-9                            8,064,951\n",
       "======================================================================\n",
       "Total params: 25,921,719\n",
       "Trainable params: 25,921,719\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "# summary(model, input_size=(16,512), dtypes=['torch.IntTensor'], device='cpu')\n",
    "summary(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7d4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1007ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence):\n",
    "  # get encoder output first\n",
    "    enc_input = tokenizer(input_sentence, return_tensors='pt').to(device)\n",
    "    #print(device)\n",
    "    enc_output = encoder(enc_input['input_ids'], enc_input['attention_mask'])\n",
    "\n",
    "    # setup initial decoder input\n",
    "    dec_input_ids = torch.tensor([[ int(tokenizer.vocab_size)]], device=device)\n",
    "    dec_attn_mask = torch.ones_like(dec_input_ids, device=device)\n",
    "\n",
    "  # now do the decoder loop\n",
    "    for _ in range(32):\n",
    "        dec_output = decoder(\n",
    "            enc_output,\n",
    "            dec_input_ids,\n",
    "            enc_input['attention_mask'],\n",
    "            dec_attn_mask,\n",
    "        )\n",
    "\n",
    "        # choose the best value (or sample)\n",
    "        prediction_id = torch.argmax(dec_output[:, -1, :], axis=-1)\n",
    "\n",
    "        # append to decoder input\n",
    "        dec_input_ids = torch.hstack((dec_input_ids, prediction_id.view(1, 1)))\n",
    "\n",
    "        # recreate mask\n",
    "        dec_attn_mask = torch.ones_like(dec_input_ids)\n",
    "\n",
    "        # exit when reach </s>\n",
    "        if prediction_id == 0:\n",
    "            break\n",
    "  \n",
    "    translation = tokenizer.decode(dec_input_ids[0, 1:])\n",
    "    #print(translation)\n",
    "    return(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba159fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(),lr=1e-5)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b48ce324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14172/3816762898.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleu_metric = load_metric(\"sacrebleu\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "bleu_metric = load_metric(\"sacrebleu\")\n",
    "bert_metric = load_metric(\"bertscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a65b52",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da2231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa93ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# A function to encapsulate the training loop\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, valid_loader, epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "\n",
    "    for it in range(epochs):\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        for batch in train_loader:\n",
    "            # move data to GPU (enc_input, enc_mask, translation)\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            enc_input = batch['input_ids']\n",
    "            enc_mask = batch['attention_mask']\n",
    "            targets = batch['labels']\n",
    "\n",
    "            # shift targets forwards to get decoder_input\n",
    "            dec_input = targets.clone().detach()\n",
    "            dec_input = torch.roll(dec_input, shifts=1, dims=1)\n",
    "            dec_input[:, 0] = int(tokenizer.vocab_size)\n",
    "\n",
    "            # also convert all -100 to pad token id\n",
    "            dec_input = dec_input.masked_fill(\n",
    "              dec_input == -100, tokenizer.pad_token_id)\n",
    "\n",
    "            # make decoder input mask\n",
    "            dec_mask = torch.ones_like(dec_input)\n",
    "            dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "                loss = criterion(outputs.transpose(2, 1), targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            \n",
    "            scaler.update()\n",
    "            #scheduler.step(optimizer)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        # Get train loss and test loss\n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        for batch in valid_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            enc_input = batch['input_ids']\n",
    "            enc_mask = batch['attention_mask']\n",
    "            targets = batch['labels']\n",
    "\n",
    "            # shift targets forwards to get decoder_input\n",
    "            dec_input = targets.clone().detach()\n",
    "            dec_input = torch.roll(dec_input, shifts=1, dims=1)\n",
    "            dec_input[:, 0] = int(tokenizer.vocab_size)\n",
    "\n",
    "            # change -100s to regular padding\n",
    "            dec_input = dec_input.masked_fill(\n",
    "              dec_input == -100, tokenizer.pad_token_id)\n",
    "\n",
    "            # make decoder input mask\n",
    "            dec_mask = torch.ones_like(dec_input)\n",
    "            dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "\n",
    "            outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "            #with torch.amp.autocast(device_type=“cuda”, dtype=torch.float16):\n",
    "            loss = criterion(outputs.transpose(2, 1), targets)\n",
    "            test_loss.append(loss.item())\n",
    "        test_loss = np.mean(test_loss)\n",
    "\n",
    "        # Save losses\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        translations = []\n",
    "        targets = []\n",
    "        bertscores = []\n",
    "        bleuscores = []\n",
    "        for  i in range(400):\n",
    "            translations.append(translate(split['test'][i][l1])[:-4])\n",
    "            targets.append(split['test'][i][l2])\n",
    "            score =bert_metric.compute(predictions=[translations[i]], references=[targets[i]], lang=\"ru\")['f1']\n",
    "            bertscores.append(score)\n",
    "            score =bleu_metric.compute(predictions=[translations[i]], references=[[targets[i]]])['score']\n",
    "            bleuscores.append(score)\n",
    "        lscheduler.step()\n",
    "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "          Test Loss: {test_loss:.4f}, Duration: {dt}')\n",
    "        print(f\"Mean BERT {np.mean(bertscores)}, Mean Bleu: {np.mean(bleuscores)}\" )\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b6540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75140e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses = train(\n",
    "    transformer, criterion, optimizer, train_loader, valid_loader, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f566a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), \"en_ru_transformer_16epochs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db3e6942",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "targets = []\n",
    "bertscores = []\n",
    "bleuscores = []\n",
    "for  i in range(1000):\n",
    "    translations.append(translate(split['test'][i][l1])[:-4])\n",
    "    targets.append(split['test'][i][l2])\n",
    "    score =bert_metric.compute(predictions=[translations[i]], references=[targets[i]], lang=\"ru\")['f1']\n",
    "    bertscores.append(score)\n",
    "    score =bleu_metric.compute(predictions=[translations[i]], references=[[targets[i]]])['score']\n",
    "    bleuscores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f56f25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9134581453204155"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bertscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82340e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.68201343735392"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bleuscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6746aaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Том презирал Мэри.\n",
      "Том презирал Мэри.\n",
      "\n",
      "Он обещал, что сделает это.\n",
      "Он обещал, что сделает это.\n",
      "\n",
      "Ты похож на довольны.\n",
      "Ты выглядишь довольной.\n",
      "\n",
      "Как была твой урок?\n",
      "Как прошло занятие?\n",
      "\n",
      "Кто вызвал Анна?\n",
      "Кто звонил Энн?\n",
      "\n",
      "Займи его в сух!\n",
      "Заточить его в темницу!\n",
      "\n",
      "Чего ещё Том хочет?\n",
      "Чего ещё Том хочет?\n",
      "\n",
      "Том ушёл раньше.\n",
      "Том ушёл раньше.\n",
      "\n",
      "Том слишком молод, чтобы голосовать.\n",
      "Тому рано голосовать.\n",
      "\n",
      "Мы найдём решение, я уверен.\n",
      "Мы найдём решение, я уверен.\n",
      "\n",
      "Когда твой первый класс?\n",
      "Когда у вас первый урок?\n",
      "\n",
      "Я видел, как Том и Мэри держались руки.\n",
      "Я видел, как Том с Мэри держались за руки.\n",
      "\n",
      "Не говори мне, что я уже знаю.\n",
      "Не говорите мне того, что я и так знаю.\n",
      "\n",
      "Если бы женщины знали, сколько бы мы по нему много раз скучали, они бы ушли.\n",
      "Если бы женщины знали, как мы по ним скучаем, они бы раньше уходили.\n",
      "\n",
      "Откуда ты знаешь, что это неправда?\n",
      "Откуда вы знаете, что это неправда?\n",
      "\n",
      "Не говори с автобусом, пока он едет на автобусе.\n",
      "Не разговаривайте с водителем автобуса во время движения.\n",
      "\n",
      "Спокоядь Тома удивил меня.\n",
      "Молчание Тома меня удивило.\n",
      "\n",
      "Где находится ближайший автобус?\n",
      "Где ближайшая автобусная остановка?\n",
      "\n",
      "У меня никогда раньше не было болит болит болит.\n",
      "У меня ещё никогда не было такой сильной головной боли.\n",
      "\n",
      "Можно воспользоваться вашей туалетом?\n",
      "Можно воспользоваться твоим туалетом?\n",
      "\n",
      "Том сказал, что надеется, что вы это сделаете.\n",
      "Том сказал, что надеется, что вы это сделаете.\n",
      "\n",
      "Том и Мэри изучают то же группе.\n",
      "Том и Мэри в одной учебной группе.\n",
      "\n",
      "Я сегодня очень устал.\n",
      "Сегодня я очень устала.\n",
      "\n",
      "Я хочу понять Тома.\n",
      "Я хочу понять Тома.\n",
      "\n",
      "Моя нога опять спит!\n",
      "Нога опять затекла.\n",
      "\n",
      "Это я преподавал Тома французский.\n",
      "Это я учу Тома французскому.\n",
      "\n",
      "Какого варианты?\n",
      "Какие есть варианты?\n",
      "\n",
      "Том посмотрел.\n",
      "Том посмотрел наверх.\n",
      "\n",
      "Он не выглядит очень счастливым.\n",
      "Он не выглядит особенно счастливым.\n",
      "\n",
      "Половора яблоко был галстот.\n",
      "Половина яблока была гнилой.\n",
      "\n",
      "Ты странный человек.\n",
      "Ты странный человек.\n",
      "\n",
      "Тебе, наверное, не стоит говорить Тому, чтобы он это сделал.\n",
      "Тебе, наверное, не стоит говорить Тому, чтобы он это сделал.\n",
      "\n",
      "Я не знал, что ты умеешь играть на кахбоне.\n",
      "Я не знал, что ты умеешь играть на тромбоне.\n",
      "\n",
      "На меня играли мало животных.\n",
      "Пара зверей играла на лугу.\n",
      "\n",
      "Ремобиль называют эту реку \"сидку и страх.\n",
      "Местные жители называют эту реку \"людоедом\" и боятся её.\n",
      "\n",
      "В этом сайте много идей.\n",
      "На этом сайте много полезных статей.\n",
      "\n",
      "Я решил туда не ходить.\n",
      "Я решил туда не ходить.\n",
      "\n",
      "Мы с Томом оба едем.\n",
      "Мы с Томом оба едем.\n",
      "\n",
      "Как нам это снова предотвратить?\n",
      "Как нам предотвратить это в дальнейшем?\n",
      "\n",
      "Вы будете ждать?\n",
      "Будете ждать?\n",
      "\n",
      "Это заставилось мои волосы на конце концов.\n",
      "У меня от этого волосы встали дыбом.\n",
      "\n",
      "Это не хохохо.\n",
      "Это не учение.\n",
      "\n",
      "Том громко закричал.\n",
      "Том громко кашлянул.\n",
      "\n",
      "Что ты всё одет?\n",
      "Куда это вы все так вырядились?\n",
      "\n",
      "Она слишком богата, чтобы стать вором.\n",
      "Она слишком богата, чтобы становиться вором.\n",
      "\n",
      "Что это говорят Библию?\n",
      "Что Библия говорит по этому поводу?\n",
      "\n",
      "Том - хиж.\n",
      "Том - крупье.\n",
      "\n",
      "Это хватит довольно умный.\n",
      "Тот продавец выглядит достаточно умным.\n",
      "\n",
      "Я с Томом врезал машину.\n",
      "Я еду с Томом в одной машине.\n",
      "\n",
      "Не мешайте её.\n",
      "Не поощряйте её.\n",
      "\n",
      "Почему Джейн поехала на станции?\n",
      "Зачем Джейн поехала на вокзал?\n",
      "\n",
      "Не думаю, что Том захочет с вами в Бостон.\n",
      "Я не думаю, что Том захочет поехать с тобой в Бостон.\n",
      "\n",
      "Мэри в красивой красной платье.\n",
      "На Мэри красивое красное платье.\n",
      "\n",
      "Хотел бы я быть таким высоким, как ты.\n",
      "Жаль, что я не такой высокий, как ты.\n",
      "\n",
      "Невежливо уставиться.\n",
      "Глазеть не вежливо.\n",
      "\n",
      "Это чувство себя глупо.\n",
      "Эта ткань гладкая на ощупь.\n",
      "\n",
      "Том не такой молод.\n",
      "Том не такой уж и молодой.\n",
      "\n",
      "Удивки сейчас из моды.\n",
      "Длинные юбки вышли из моды.\n",
      "\n",
      "Я не смогу учиться без этой книги.\n",
      "Без этой книги я не смогу учиться.\n",
      "\n",
      "Не смотри в коробку.\n",
      "Не заглядывайте в ящик.\n",
      "\n",
      "Всем нужно себя любить.\n",
      "Каждому необходимо чувствовать себя любимым.\n",
      "\n",
      "Я бы лучше умрю тебя замуж.\n",
      "Я скорее умру, чем женюсь на тебе.\n",
      "\n",
      "Кто-то хочет меня убить.\n",
      "Кто-то хочет меня убить.\n",
      "\n",
      "Я делаю магазин в рынке.\n",
      "Я делаю покупки на рынке.\n",
      "\n",
      "Музыка имеет значение.\n",
      "Размер имеет значение.\n",
      "\n",
      "Вы уже ели дома?\n",
      "Ты уже поела дома?\n",
      "\n",
      "Откуда ты знал, что Том болен?\n",
      "Откуда ты узнал, что Том болен?\n",
      "\n",
      "С кем ты разговаривал?\n",
      "С кем вы разговаривали?\n",
      "\n",
      "Том хватал два дня.\n",
      "Том два дня репетировал.\n",
      "\n",
      "Ты ведь не спешишь?\n",
      "Вы же не торопитесь?\n",
      "\n",
      "Мне надо было тебе позвонить.\n",
      "Мне надо было тебе позвонить.\n",
      "\n",
      "Вы не знали, что Том спит?\n",
      "Вы не знали, что Том спит?\n",
      "\n",
      "Он побледал с хорошо.\n",
      "Он побледнел от испуга.\n",
      "\n",
      "Она обещала.\n",
      "Она обещала.\n",
      "\n",
      "Мэри, похоже, не убедила, что ей надо это сделать.\n",
      "Непохоже, что Мария уверена, что ей нужно делать это.\n",
      "\n",
      "Не думаю, что Том когда-нибудь сможет снова ходить.\n",
      "Не думаю, что Том когда-нибудь сможет опять ходить.\n",
      "\n",
      "У тебя есть?\n",
      "Он у вас?\n",
      "\n",
      "Я пытался тебя защитить.\n",
      "Я пытался защитить вас.\n",
      "\n",
      "Ваш письмо слишком поздно пришёл.\n",
      "Твоё письмо пришло слишком поздно.\n",
      "\n",
      "Я думаю, Том меня болен.\n",
      "Думаю, Тома от меня тошнит.\n",
      "\n",
      "Человек вызвался, когда ты вышел.\n",
      "Пока тебя не было, приходил какой-то Джонс.\n",
      "\n",
      "Какие книги вы читаете?\n",
      "Какого рода книги ты читаешь?\n",
      "\n",
      "Он знает, что ты знаешь?\n",
      "Он знает, что ты знаешь?\n",
      "\n",
      "Я не могу вас ждать.\n",
      "Мне не терпится тебя увидеть.\n",
      "\n",
      "Никто обо мне не заботится.\n",
      "На меня всем наплевать.\n",
      "\n",
      "Я слышал, он умер.\n",
      "Я слышал, что он умер.\n",
      "\n",
      "У кого есть главная роль?\n",
      "Кто в главной роли?\n",
      "\n",
      "Бедная девушка была в том, чтобы быть смерти.\n",
      "Бедная девушка была при смерти.\n",
      "\n",
      "Том сегодня приехал домой позже обычного.\n",
      "Сегодня Том добрался до дома позже чем обычно.\n",
      "\n",
      "Думаю, я знаю, что теперь случится.\n",
      "Кажется, я знаю, что сейчас будет.\n",
      "\n",
      "Пожалуйста, ходите в банк.\n",
      "Пожалуйста, сходи в банк.\n",
      "\n",
      "Не заставляйте его.\n",
      "Не поднимайте его.\n",
      "\n",
      "Ты не прав.\n",
      "Ты ошибаешься.\n",
      "\n",
      "Нет, небо не будет на нашу терпею.\n",
      "Нет, небо не упадёт нам на голову.\n",
      "\n",
      "Тебе надо было попросить у Тома о помощи.\n",
      "Тебе надо было попросить у Тома прощения за то, что ты его обозвал.\n",
      "\n",
      "Город - три горы из дерева.\n",
      "Город находится в трёх милях от того места.\n",
      "\n",
      "Принесите ребёнка трудно работать.\n",
      "Воспитание детей — тяжелый труд.\n",
      "\n",
      "Не разочаруйся в обезьяне.\n",
      "Не злоупотребляйте восклицательными знаками.\n",
      "\n",
      "Том купил кусочек кусочки недалеко, где живёт Мэри.\n",
      "Том купил участок земли недалеко от того места, где живёт Мэри.\n",
      "\n",
      "Вы знали, что это живёт в этом горе?\n",
      "Вы знали, что на этой горе живут лисицы?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(translations)):\n",
    "    print(translations[i])\n",
    "    print(targets[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a193d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Том презирал Мэри.',\n",
       " 'Он обещал, что сделает это.',\n",
       " 'Ты выглядишь довольной.',\n",
       " 'Как прошло занятие?',\n",
       " 'Кто звонил Энн?',\n",
       " 'Заточить его в темницу!',\n",
       " 'Чего ещё Том хочет?',\n",
       " 'Том ушёл раньше.',\n",
       " 'Тому рано голосовать.',\n",
       " 'Мы найдём решение, я уверен.',\n",
       " 'Когда у вас первый урок?',\n",
       " 'Я видел, как Том с Мэри держались за руки.',\n",
       " 'Не говорите мне того, что я и так знаю.',\n",
       " 'Если бы женщины знали, как мы по ним скучаем, они бы раньше уходили.',\n",
       " 'Откуда вы знаете, что это неправда?',\n",
       " 'Не разговаривайте с водителем автобуса во время движения.',\n",
       " 'Молчание Тома меня удивило.',\n",
       " 'Где ближайшая автобусная остановка?',\n",
       " 'У меня ещё никогда не было такой сильной головной боли.',\n",
       " 'Можно воспользоваться твоим туалетом?',\n",
       " 'Том сказал, что надеется, что вы это сделаете.',\n",
       " 'Том и Мэри в одной учебной группе.',\n",
       " 'Сегодня я очень устала.',\n",
       " 'Я хочу понять Тома.',\n",
       " 'Нога опять затекла.',\n",
       " 'Это я учу Тома французскому.',\n",
       " 'Какие есть варианты?',\n",
       " 'Том посмотрел наверх.',\n",
       " 'Он не выглядит особенно счастливым.',\n",
       " 'Половина яблока была гнилой.',\n",
       " 'Ты странный человек.',\n",
       " 'Тебе, наверное, не стоит говорить Тому, чтобы он это сделал.',\n",
       " 'Я не знал, что ты умеешь играть на тромбоне.',\n",
       " 'Пара зверей играла на лугу.',\n",
       " 'Местные жители называют эту реку \"людоедом\" и боятся её.',\n",
       " 'На этом сайте много полезных статей.',\n",
       " 'Я решил туда не ходить.',\n",
       " 'Мы с Томом оба едем.',\n",
       " 'Как нам предотвратить это в дальнейшем?',\n",
       " 'Будете ждать?',\n",
       " 'У меня от этого волосы встали дыбом.',\n",
       " 'Это не учение.',\n",
       " 'Том громко кашлянул.',\n",
       " 'Куда это вы все так вырядились?',\n",
       " 'Она слишком богата, чтобы становиться вором.',\n",
       " 'Что Библия говорит по этому поводу?',\n",
       " 'Том - крупье.',\n",
       " 'Тот продавец выглядит достаточно умным.',\n",
       " 'Я еду с Томом в одной машине.',\n",
       " 'Не поощряйте её.',\n",
       " 'Зачем Джейн поехала на вокзал?',\n",
       " 'Я не думаю, что Том захочет поехать с тобой в Бостон.',\n",
       " 'На Мэри красивое красное платье.',\n",
       " 'Жаль, что я не такой высокий, как ты.',\n",
       " 'Глазеть не вежливо.',\n",
       " 'Эта ткань гладкая на ощупь.',\n",
       " 'Том не такой уж и молодой.',\n",
       " 'Длинные юбки вышли из моды.',\n",
       " 'Без этой книги я не смогу учиться.',\n",
       " 'Не заглядывайте в ящик.',\n",
       " 'Каждому необходимо чувствовать себя любимым.',\n",
       " 'Я скорее умру, чем женюсь на тебе.',\n",
       " 'Кто-то хочет меня убить.',\n",
       " 'Я делаю покупки на рынке.',\n",
       " 'Размер имеет значение.',\n",
       " 'Ты уже поела дома?',\n",
       " 'Откуда ты узнал, что Том болен?',\n",
       " 'С кем вы разговаривали?',\n",
       " 'Том два дня репетировал.',\n",
       " 'Вы же не торопитесь?',\n",
       " 'Мне надо было тебе позвонить.',\n",
       " 'Вы не знали, что Том спит?',\n",
       " 'Он побледнел от испуга.',\n",
       " 'Она обещала.',\n",
       " 'Непохоже, что Мария уверена, что ей нужно делать это.',\n",
       " 'Не думаю, что Том когда-нибудь сможет опять ходить.',\n",
       " 'Он у вас?',\n",
       " 'Я пытался защитить вас.',\n",
       " 'Твоё письмо пришло слишком поздно.',\n",
       " 'Думаю, Тома от меня тошнит.',\n",
       " 'Пока тебя не было, приходил какой-то Джонс.',\n",
       " 'Какого рода книги ты читаешь?',\n",
       " 'Он знает, что ты знаешь?',\n",
       " 'Мне не терпится тебя увидеть.',\n",
       " 'На меня всем наплевать.',\n",
       " 'Я слышал, что он умер.',\n",
       " 'Кто в главной роли?',\n",
       " 'Бедная девушка была при смерти.',\n",
       " 'Сегодня Том добрался до дома позже чем обычно.',\n",
       " 'Кажется, я знаю, что сейчас будет.',\n",
       " 'Пожалуйста, сходи в банк.',\n",
       " 'Не поднимайте его.',\n",
       " 'Ты ошибаешься.',\n",
       " 'Нет, небо не упадёт нам на голову.',\n",
       " 'Тебе надо было попросить у Тома прощения за то, что ты его обозвал.',\n",
       " 'Город находится в трёх милях от того места.',\n",
       " 'Воспитание детей — тяжелый труд.',\n",
       " 'Не злоупотребляйте восклицательными знаками.',\n",
       " 'Том купил участок земли недалеко от того места, где живёт Мэри.',\n",
       " 'Вы знали, что на этой горе живут лисицы?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3412bdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/sutclw/anaconda3/envs/torch/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "backuplogs\t\t      logs\t     translation\r\n",
      "build\t\t\t      main.py\t     translation.egg-info\r\n",
      "Encoder_Decoder_ru.ipynb      notebooks      Translation.ipynb\r\n",
      "en_ru_transformer_8epochs.pt  setup.py\t     Untitled.ipynb\r\n",
      "Evaluation.ipynb\t      test.csv\t     WordStressTransformer.ipynb\r\n",
      "Evaluation_pytorch.ipynb      Testing.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91aa2f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(62519, 64)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_blocks): Sequential(\n",
       "      (0): EncoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): EncoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(62519, 64)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_blocks): Sequential(\n",
       "      (0): DecoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha1): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (mha2): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha1): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (mha2): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha1): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (mha2): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha1): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (mha2): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc): Linear(in_features=64, out_features=62519, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_state_dict(torch.load('en_ru_transformer_8epochs.pt'))\n",
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "448365bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "targets = []\n",
    "bertscores = []\n",
    "bleuscores = []\n",
    "for  i in range(100):\n",
    "    translations.append(translate(split['test'][i][l1])[:-4])\n",
    "    targets.append(split['test'][i][l2])\n",
    "    score =bert_metric.compute(predictions=[translations[i]], references=[targets[i]], lang=\"ru\")['f1']\n",
    "    bertscores.append(score)\n",
    "    score =bleu_metric.compute(predictions=[translations[i]], references=[[targets[i]]])['score']\n",
    "    bleuscores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9baa6a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Том презирал Мэри.',\n",
       " 'Он обещал, что сделает это.',\n",
       " 'Ты похож на довольны.',\n",
       " 'Как была твой урок?',\n",
       " 'Кто вызвал Анна?',\n",
       " 'Займи его в сух!',\n",
       " 'Чего ещё Том хочет?',\n",
       " 'Том ушёл раньше.',\n",
       " 'Том слишком молод, чтобы голосовать.',\n",
       " 'Мы найдём решение, я уверен.',\n",
       " 'Когда твой первый класс?',\n",
       " 'Я видел, как Том и Мэри держались руки.',\n",
       " 'Не говори мне, что я уже знаю.',\n",
       " 'Если бы женщины знали, сколько бы мы по нему много раз скучали, они бы ушли.',\n",
       " 'Откуда ты знаешь, что это неправда?',\n",
       " 'Не говори с автобусом, пока он едет на автобусе.',\n",
       " 'Спокоядь Тома удивил меня.',\n",
       " 'Где находится ближайший автобус?',\n",
       " 'У меня никогда раньше не было болит болит болит.',\n",
       " 'Можно воспользоваться вашей туалетом?',\n",
       " 'Том сказал, что надеется, что вы это сделаете.',\n",
       " 'Том и Мэри изучают то же группе.',\n",
       " 'Я сегодня очень устал.',\n",
       " 'Я хочу понять Тома.',\n",
       " 'Моя нога опять спит!',\n",
       " 'Это я преподавал Тома французский.',\n",
       " 'Какого варианты?',\n",
       " 'Том посмотрел.',\n",
       " 'Он не выглядит очень счастливым.',\n",
       " 'Половора яблоко был галстот.',\n",
       " 'Ты странный человек.',\n",
       " 'Тебе, наверное, не стоит говорить Тому, чтобы он это сделал.',\n",
       " 'Я не знал, что ты умеешь играть на кахбоне.',\n",
       " 'На меня играли мало животных.',\n",
       " 'Ремобиль называют эту реку \"сидку и страх.',\n",
       " 'В этом сайте много идей.',\n",
       " 'Я решил туда не ходить.',\n",
       " 'Мы с Томом оба едем.',\n",
       " 'Как нам это снова предотвратить?',\n",
       " 'Вы будете ждать?',\n",
       " 'Это заставилось мои волосы на конце концов.',\n",
       " 'Это не хохохо.',\n",
       " 'Том громко закричал.',\n",
       " 'Что ты всё одет?',\n",
       " 'Она слишком богата, чтобы стать вором.',\n",
       " 'Что это говорят Библию?',\n",
       " 'Том - хиж.',\n",
       " 'Это хватит довольно умный.',\n",
       " 'Я с Томом врезал машину.',\n",
       " 'Не мешайте её.',\n",
       " 'Почему Джейн поехала на станции?',\n",
       " 'Не думаю, что Том захочет с вами в Бостон.',\n",
       " 'Мэри в красивой красной платье.',\n",
       " 'Хотел бы я быть таким высоким, как ты.',\n",
       " 'Невежливо уставиться.',\n",
       " 'Это чувство себя глупо.',\n",
       " 'Том не такой молод.',\n",
       " 'Удивки сейчас из моды.',\n",
       " 'Я не смогу учиться без этой книги.',\n",
       " 'Не смотри в коробку.',\n",
       " 'Всем нужно себя любить.',\n",
       " 'Я бы лучше умрю тебя замуж.',\n",
       " 'Кто-то хочет меня убить.',\n",
       " 'Я делаю магазин в рынке.',\n",
       " 'Музыка имеет значение.',\n",
       " 'Вы уже ели дома?',\n",
       " 'Откуда ты знал, что Том болен?',\n",
       " 'С кем ты разговаривал?',\n",
       " 'Том хватал два дня.',\n",
       " 'Ты ведь не спешишь?',\n",
       " 'Мне надо было тебе позвонить.',\n",
       " 'Вы не знали, что Том спит?',\n",
       " 'Он побледал с хорошо.',\n",
       " 'Она обещала.',\n",
       " 'Мэри, похоже, не убедила, что ей надо это сделать.',\n",
       " 'Не думаю, что Том когда-нибудь сможет снова ходить.',\n",
       " 'У тебя есть?',\n",
       " 'Я пытался тебя защитить.',\n",
       " 'Ваш письмо слишком поздно пришёл.',\n",
       " 'Я думаю, Том меня болен.',\n",
       " 'Человек вызвался, когда ты вышел.',\n",
       " 'Какие книги вы читаете?',\n",
       " 'Он знает, что ты знаешь?',\n",
       " 'Я не могу вас ждать.',\n",
       " 'Никто обо мне не заботится.',\n",
       " 'Я слышал, он умер.',\n",
       " 'У кого есть главная роль?',\n",
       " 'Бедная девушка была в том, чтобы быть смерти.',\n",
       " 'Том сегодня приехал домой позже обычного.',\n",
       " 'Думаю, я знаю, что теперь случится.',\n",
       " 'Пожалуйста, ходите в банк.',\n",
       " 'Не заставляйте его.',\n",
       " 'Ты не прав.',\n",
       " 'Нет, небо не будет на нашу терпею.',\n",
       " 'Тебе надо было попросить у Тома о помощи.',\n",
       " 'Город - три горы из дерева.',\n",
       " 'Принесите ребёнка трудно работать.',\n",
       " 'Не разочаруйся в обезьяне.',\n",
       " 'Том купил кусочек кусочки недалеко, где живёт Мэри.',\n",
       " 'Вы знали, что это живёт в этом горе?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c013b0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'私は<unk> が好きです<unk> </s>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"I like rice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a60972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    n_h = trial.suggest_int(\"n_heads\", 4, 8, 1)\n",
    "    n_l = trial.suggest_int(\"n_layers\", 2, 8, 1)\n",
    "    #n_mod = trial.suggest_int(\"n_layers\", 64, 128, 64)\n",
    "   \n",
    "    encoder = Encoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                      max_len=512,\n",
    "                      d_k=16,\n",
    "                      d_model=128,\n",
    "                      n_heads=n_h,\n",
    "                      n_layers=n_l,\n",
    "                      dropout_prob=0.1)\n",
    "    decoder = Decoder(vocab_size=tokenizer.vocab_size + 1,\n",
    "                      max_len=512,\n",
    "                      d_k=16,\n",
    "                      d_model=128,\n",
    "                      n_heads=n_h,\n",
    "                      n_layers=n_l,\n",
    "                      dropout_prob=0.1)\n",
    "    transformer = Transformer(encoder, decoder)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    return(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b36214dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence, model):\n",
    "  # get encoder output first\n",
    "    enc_input = tokenizer(input_sentence, return_tensors='pt').to(device)\n",
    "    #print(device)\n",
    "    enc_output = model.encoder(enc_input['input_ids'], enc_input['attention_mask'])\n",
    "\n",
    "    # setup initial decoder input\n",
    "    dec_input_ids = torch.tensor([[ int(tokenizer.vocab_size)]], device=device)\n",
    "    dec_attn_mask = torch.ones_like(dec_input_ids, device=device)\n",
    "\n",
    "  # now do the decoder loop\n",
    "    for _ in range(32):\n",
    "        dec_output = model.decoder(\n",
    "            enc_output,\n",
    "            dec_input_ids,\n",
    "            enc_input['attention_mask'],\n",
    "            dec_attn_mask,\n",
    "        )\n",
    "\n",
    "        # choose the best value (or sample)\n",
    "        prediction_id = torch.argmax(dec_output[:, -1, :], axis=-1)\n",
    "\n",
    "        # append to decoder input\n",
    "        dec_input_ids = torch.hstack((dec_input_ids, prediction_id.view(1, 1)))\n",
    "\n",
    "        # recreate mask\n",
    "        dec_attn_mask = torch.ones_like(dec_input_ids)\n",
    "\n",
    "        # exit when reach </s>\n",
    "        if prediction_id == 0:\n",
    "            break\n",
    "  \n",
    "    translation = tokenizer.decode(dec_input_ids[0, 1:])\n",
    "    #print(translation)\n",
    "    return(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12261e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60deab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "device='cuda'\n",
    "def objective(trial,epochs=3):\n",
    "    # Generate the model.\n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "    model = define_model(trial).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    for it in range(epochs):\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        for batch in train_loader:\n",
    "            # move data to GPU (enc_input, enc_mask, translation)\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            enc_input = batch['input_ids']\n",
    "            enc_mask = batch['attention_mask']\n",
    "            targets = batch['labels']\n",
    "\n",
    "            # shift targets forwards to get decoder_input\n",
    "            dec_input = targets.clone().detach()\n",
    "            dec_input = torch.roll(dec_input, shifts=1, dims=1)\n",
    "            dec_input[:, 0] = int(tokenizer.vocab_size)\n",
    "\n",
    "            # also convert all -100 to pad token id\n",
    "            dec_input = dec_input.masked_fill(\n",
    "              dec_input == -100, tokenizer.pad_token_id)\n",
    "\n",
    "            # make decoder input mask\n",
    "            dec_mask = torch.ones_like(dec_input)\n",
    "            dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "                loss = criterion(outputs.transpose(2, 1), targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        # Get train loss and test loss\n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        for batch in valid_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            enc_input = batch['input_ids']\n",
    "            enc_mask = batch['attention_mask']\n",
    "            targets = batch['labels']\n",
    "\n",
    "            # shift targets forwards to get decoder_input\n",
    "            dec_input = targets.clone().detach()\n",
    "            dec_input = torch.roll(dec_input, shifts=1, dims=1)\n",
    "            dec_input[:, 0] = int(tokenizer.vocab_size)\n",
    "\n",
    "            # change -100s to regular padding\n",
    "            dec_input = dec_input.masked_fill(\n",
    "              dec_input == -100, tokenizer.pad_token_id)\n",
    "\n",
    "            # make decoder input mask\n",
    "            dec_mask = torch.ones_like(dec_input)\n",
    "            dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "\n",
    "            outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "            #with torch.amp.autocast(device_type=“cuda”, dtype=torch.float16):\n",
    "            loss = criterion(outputs.transpose(2, 1), targets)\n",
    "            test_loss.append(loss.item())\n",
    "            \n",
    "            \n",
    "        translations=[]    \n",
    "        targets = []\n",
    "        bertscores = []\n",
    "        bleuscores = []\n",
    "        for  i in range(200):\n",
    "            translations.append(translate(split['test'][i][l1], model)[:-4])\n",
    "            targets.append(split['test'][i][l2])\n",
    "            score =bert_metric.compute(predictions=[translations[i]], references=[targets[i]], lang=\"ru\")['f1']\n",
    "            bertscores.append(score)\n",
    "            score =bleu_metric.compute(predictions=[translations[i]], references=[[targets[i]]])['score']\n",
    "            bleuscores.append(score)\n",
    "        print(f\"Mean BERT {np.mean(bertscores)}, Mean Bleu: {np.mean(bleuscores)}\" )\n",
    "        test_loss = np.mean(test_loss)\n",
    "\n",
    "        # Save losses\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "\n",
    "\n",
    "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "          Test Loss: {test_loss:.4f}, Duration: {dt}')\n",
    "    return(np.mean(bertscores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61d51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "162a3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0107107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-03-01 20:19:43,969]\u001B[0m A new study created in memory with name: no-name-268307ed-45a7-42d7-811c-7c4abee954ad\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33m[W 2023-03-01 20:20:32,293]\u001B[0m Trial 0 failed with parameters: {'n_heads': 7, 'n_layers': 3, 'lr': 0.00012237253778222962} because of the following error: KeyboardInterrupt().\u001B[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sutclw/anaconda3/envs/torch/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_8878/1066740464.py\", line 52, in objective\n",
      "    scaler.step(optimizer)\n",
      "  File \"/home/sutclw/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py\", line 341, in step\n",
      "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
      "  File \"/home/sutclw/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py\", line 287, in _maybe_opt_step\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "  File \"/home/sutclw/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py\", line 287, in <genexpr>\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "KeyboardInterrupt\n",
      "\u001B[33m[W 2023-03-01 20:20:32,294]\u001B[0m Trial 0 failed with value None.\u001B[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mstudy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m pruned_trials \u001B[38;5;241m=\u001B[39m study\u001B[38;5;241m.\u001B[39mget_trials(deepcopy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, states\u001B[38;5;241m=\u001B[39m[TrialState\u001B[38;5;241m.\u001B[39mPRUNED])\n\u001B[1;32m      5\u001B[0m complete_trials \u001B[38;5;241m=\u001B[39m study\u001B[38;5;241m.\u001B[39mget_trials(deepcopy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, states\u001B[38;5;241m=\u001B[39m[TrialState\u001B[38;5;241m.\u001B[39mCOMPLETE])\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/optuna/study/study.py:425\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    323\u001B[0m     func: ObjectiveFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    330\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    331\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    332\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[1;32m    333\u001B[0m \n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[1;32m    423\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 425\u001B[0m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    426\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    427\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    435\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001B[0m, in \u001B[0;36m_optimize\u001B[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 66\u001B[0m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     79\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[1;32m    160\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 163\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    244\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    247\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[1;32m    248\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[1;32m    250\u001B[0m ):\n\u001B[0;32m--> 251\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 200\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    202\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[1;32m    203\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[0;32mIn[20], line 52\u001B[0m, in \u001B[0;36mobjective\u001B[0;34m(trial, epochs)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# Backward and optimize\u001B[39;00m\n\u001B[1;32m     51\u001B[0m scaler\u001B[38;5;241m.\u001B[39mscale(loss)\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 52\u001B[0m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m scaler\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[1;32m     54\u001B[0m train_loss\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:341\u001B[0m, in \u001B[0;36mGradScaler.step\u001B[0;34m(self, optimizer, *args, **kwargs)\u001B[0m\n\u001B[1;32m    337\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munscale_(optimizer)\n\u001B[1;32m    339\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf_per_device\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo inf checks were recorded for this optimizer.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 341\u001B[0m retval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_opt_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    343\u001B[0m optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstage\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m OptState\u001B[38;5;241m.\u001B[39mSTEPPED\n\u001B[1;32m    345\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:287\u001B[0m, in \u001B[0;36mGradScaler._maybe_opt_step\u001B[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001B[0m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_maybe_opt_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, optimizer, optimizer_state, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    286\u001B[0m     retval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 287\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moptimizer_state\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf_per_device\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    288\u001B[0m         retval \u001B[38;5;241m=\u001B[39m optimizer\u001B[38;5;241m.\u001B[39mstep(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:287\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_maybe_opt_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, optimizer, optimizer_state, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    286\u001B[0m     retval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 287\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28msum\u001B[39m(\u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf_per_device\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    288\u001B[0m         retval \u001B[38;5;241m=\u001B[39m optimizer\u001B[38;5;241m.\u001B[39mstep(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f992a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  3\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  3\n",
      "Best trial:\n",
      "  Value:  0.7627686563134194\n",
      "  Params: \n",
      "    n_heads: 5\n",
      "    n_layers: 5\n",
      "    lr: 0.00021099837151687136\n"
     ]
    }
   ],
   "source": [
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4df646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
